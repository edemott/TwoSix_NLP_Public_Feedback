{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data set and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this only once to download the model\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /mnt/home/bhatta73/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /mnt/home/bhatta73/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Download necessary resources (only once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading cleaned csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the filtered_comments.csv is a big file with around 1.8 million comments, we will load the data in batches to preserve memory. The below code chunk may take 10-15 minutes to run. It also ensure the dataframe we output only has unique comments and no duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also be removing rows in column \"comment\" which may be less than 10 characters. This ensures that only valid comments are present in the dataframe, reducing noise. We are also removing comments with the terms 'attached', 'attachment', as comments with such terms are pointing to a file or attachment and are not actual comments or a reflection of the sentiments behind EPA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded with shape: (1365767, 1)\n"
     ]
    }
   ],
   "source": [
    "def load_filtered_unique_comments(input_csv, chunksize=10000):\n",
    "    seen = set()\n",
    "    filtered_comments = []\n",
    "\n",
    "    for chunk in pd.read_csv(input_csv, chunksize=chunksize, usecols=['comment'], low_memory=False):\n",
    "        chunk.dropna(subset=['comment'], inplace=True)\n",
    "        chunk['comment'] = chunk['comment'].astype(str)\n",
    "        chunk.drop_duplicates(subset=['comment'], inplace=True)\n",
    "\n",
    "        # Filter out short comments\n",
    "        chunk = chunk[chunk['comment'].str.len() > 10]\n",
    "        \n",
    "        # Remove comments that contain any form of 'attached' or 'attachment'\n",
    "        pattern = r'\\b(?:attached|attachment)\\b'\n",
    "        chunk = chunk[~chunk['comment'].str.contains(pattern, case=False, na=False)]\n",
    "\n",
    "        # Remove already seen comments\n",
    "        chunk = chunk[~chunk['comment'].isin(seen)]\n",
    "\n",
    "        # Update seen set and result list\n",
    "        seen.update(chunk['comment'])\n",
    "        filtered_comments.extend(chunk['comment'].tolist())\n",
    "\n",
    "    return pd.DataFrame(filtered_comments, columns=['comment'])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_path = \"../data/filtered_comments.csv\"\n",
    "    df = load_filtered_unique_comments(input_path)\n",
    "    print(\"✅ Loaded with shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have loaded in 1,365,767 comments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "comment",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "2a42cdd9-de93-45b2-aebd-35ac5d137618",
       "rows": [
        [
         "0",
         "RE: Docket EPA-R10-OAR-2019-0710  Federal Register Number 2020-01465<br/><br/>The proposed revision of the Washington state implementation plan (SIP) by the Washington state department of ecology (Ecology) through changes to WAC 173-400 is necessary, in part, as industry and control agencies have long expressed a need for consistency in the application of air quality models for regulatory purposes.<br/><br/>Additional scrutiny of WAC 173-400-730 should be made for compliance with federal statute to ensure that the prevention of significant deterioration application meets all applicable laws, rules and regulations as they apply to new, large facilities or changes at existing large facilities that could increase air pollution.<br/><br/>In particular paragraph (3) of the regulation concerning PSD technical support documents may well be enhanced if modified to include requirements for modeling information required to support the proposed revision including input data, output data, models used, justification of model selection and at least ambient and meteorological data used to make assumption and other information relevant to the application.  The availability of this information in the preliminary stages of PSD determination would be helpful to gauge the overall effectiveness of the permit process.<br/><br/>The core mission of the Environmental Protection Agency (EPA) has been protecting human health and the environment.  The revision to this SIP and PSD process may help to further improve the quality, value or extent of providing clean and safe air, water and land for all Americans and should be considered.<br/><br/>"
        ],
        [
         "1",
         "People ask why the EPA is gutting regulations, cutting enforcement and swooning with puppy love for &quot;clean&quot; coal. Well it&#39;s because President Tiny Hands is in charge, and this is what Republican presidents do. Decade after decade. All the way back to that vacuous simp Ronald Reagan.<br/><br/>Okay... so what happens when the GOP is NOT in power? Good question. Let&#39;s look at Oregon:<br/><br/>&quot;Republican state senators in Oregon fled the state capitol on Monday in a stunt intended to derail the passage of a climate change bill supported by a majority of lawmakers for the second time in eight months.&quot; - Salon, Feb 26 2018<br/><br/>Remember: a vote against the GOP is a vote for Earth.<br/><br/>#PresidentTinyHands #TrumpSongbook #OrangeDumbsicle #YouTooWheeler<br/>"
        ],
        [
         "2",
         "Cache Valley needs access to Tier 3 gasoline AND more EV charging stations."
        ],
        [
         "3",
         "While attainment may appear to have been achieved &quot;on paper&quot; by the authorities referenced in this document, the reality of our air quality is that it is still - operationally - quite poor.  As a resident living in this area since 1984, I can attest that the last few years have been some of the worst I have experienced in air quality.  The visual smog/haze has increased and is ever more apparent during inversions than previously.  I am an avid non-motorized winter recreationalist - skiing, walking, hiking - and breathing the air during our inversions causes throat and lung irritations I have never experienced before.  Personal vehicles in the form of diesel utility trucks (&quot;pickup&quot; trucks) have increased dramatically in this valley in recent years, both in quantity and size of truck.  Any individual traveling our city streets, Logan Canyon, or US 91 can readily see the not uncommon occurrence of drivers of these vehicles purposefully discharging black, acrid exhaust upon acceleration.  Authorities have refused to limit/prohibit fall season burning of agriculture fields and ditches, causing significant pollution into October and November.  The Logan District of the National Forest Service burns slashed trees in the fall and winter, causing significant particulate matter to be released.  The density of snowmobiles and other snow vehicles have increased in the area and forest district, as have the trucks and trailers used to transport these machines into the backcountry.  The majority of these vehicles and equipment are not clean-burning, emitting a significant amount of pollution.  Our metro transit district and Utah State University have continued to invest in clean transit vehicles, but  private citizens have failed to do so.  Even the state thwarts efforts to induce private citizens to own appropriate vehicles that can reduce air pollution by proposing to increasing personal property taxes from 200-400%+ on hybrid, plug-in EV, and EV vehicles.  Cache Valley is not going to significantly increase its efforts on eliminating environmental contaminants without continued and meaningful regulatory enforcement and consequences.  It has done nothing to enforce idling restrictions.  It is common to see vehicles idling literally in front of &quot;no idling&quot; signs.  Until this county increases the education and consequences of non-adherence by private citizens, as well as corporate and government entities, we have not achieved attainment of any meaning or endurance in this county.  Do not lift the current mandates for attainment."
        ],
        [
         "4",
         "Yes.  I agree that we should commend achievements in the direction of healthy air, and Cache Valley Utah has made some progress.  But it is far from enough.  The air outside is still laden with polluted haze most of the winter.  I am 76 years old, have always been in good health, and take exercise outside walking as one of my chief fitness activities.  The improvement in air quality means for me that on days when there is some perceptible sunshine I can get the benefit of that limited good Vitamin D (etc.etc.).  But most days the sun is blocked by the aforesaid haze.   I still have to maintain three high-priced air purifiers in my home to keep the air inside healthy-- and my home is only 1700 square feet.  Please keep up the work to improve the air in Logan and environs.  I should not have to spend the bulk of my winter days like a mole in a hole at home, supplementing public funds with my own expenditures on fancy air-cleaners, in order to breathe clean air.    I also commend the Logan and Cache Valley governments for creating an excellent network of walking/hiking trails.  And we still need to get cleaner air so we can walk on them  breathing easily all year round."
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RE: Docket EPA-R10-OAR-2019-0710  Federal Regi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>People ask why the EPA is gutting regulations,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cache Valley needs access to Tier 3 gasoline A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>While attainment may appear to have been achie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yes.  I agree that we should commend achieveme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment\n",
       "0  RE: Docket EPA-R10-OAR-2019-0710  Federal Regi...\n",
       "1  People ask why the EPA is gutting regulations,...\n",
       "2  Cache Valley needs access to Tier 3 gasoline A...\n",
       "3  While attainment may appear to have been achie...\n",
       "4  Yes.  I agree that we should commend achieveme..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below cleans the extracted text, as it may contain html entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m     text = re.sub(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms+\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m, text).strip()  \u001b[38;5;66;03m# Remove extra spaces\u001b[39;00m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mcleaned_comment\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcomment\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_text\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/pandas/core/series.py:4924\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mapply\u001b[39m(\n\u001b[32m   4790\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4791\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4796\u001b[39m     **kwargs,\n\u001b[32m   4797\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4798\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4799\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4800\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4915\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4916\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4918\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4922\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4924\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/pandas/core/apply.py:1427\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1426\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/pandas/core/apply.py:1507\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1501\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1503\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1504\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1505\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1506\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1507\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1509\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1512\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1513\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1514\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/pandas/core/base.py:921\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    919\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/pandas/core/algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mlib.pyx:2972\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mclean_text\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      6\u001b[39m text = re.sub(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttps?://\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mS+|www\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mS+\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, text)  \u001b[38;5;66;03m# Remove URLs\u001b[39;00m\n\u001b[32m      7\u001b[39m text = re.sub(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mu[0-9A-Fa-f]\u001b[39m\u001b[38;5;132;01m{4}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, text)  \u001b[38;5;66;03m# Remove unicode escape sequences like \\u2019\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m text = \u001b[43mre\u001b[49m\u001b[43m.\u001b[49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m[^\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mw\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43ms.,!?\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Remove special characters except common punctuation\u001b[39;00m\n\u001b[32m      9\u001b[39m text = re.sub(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms+\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m, text).strip()  \u001b[38;5;66;03m# Remove extra spaces\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/CMSE495-TwoSix/lib/python3.11/re/__init__.py:185\u001b[39m, in \u001b[36msub\u001b[39m\u001b[34m(pattern, repl, string, count, flags)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34msub\u001b[39m(pattern, repl, string, count=\u001b[32m0\u001b[39m, flags=\u001b[32m0\u001b[39m):\n\u001b[32m    179\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[32m    180\u001b[39m \u001b[33;03m    non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[32m    181\u001b[39m \u001b[33;03m    replacement repl.  repl can be either a string or a callable;\u001b[39;00m\n\u001b[32m    182\u001b[39m \u001b[33;03m    if a string, backslash escapes in it are processed.  If it is\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[33;03m    a callable, it's passed the Match object and must return\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[33;03m    a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Cleans comment text by removing unicode, HTML entities, URLs, and extra spaces.\"\"\"\n",
    "    text = re.sub(r\"&#\\d+;\", \" \", text)  # Remove HTML entities like &#160;\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)  # Remove HTML tags\n",
    "    text = re.sub(r\"&[a-zA-Z]+;\", \" \", text)  # Remove HTML entities like &rsquo;, &amp;\n",
    "    text = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"\\\\u[0-9A-Fa-f]{4}\", \"\", text)  # Remove unicode escape sequences like \\u2019\n",
    "    text = re.sub(r\"[^\\w\\s.,!?']\", \"\", text)  # Remove special characters except common punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "df[\"cleaned_comment\"] = df[\"comment\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better extract sentiments from the data before putting them through BERT, let's sentencize the comments and remove filler words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Optional: load spacy for smarter tokenization if needed\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def sentencize_and_filter(comment):\n",
    "    \"\"\"Split a comment into sentences, remove stopwords, and return clean sentences.\"\"\"\n",
    "    sentences = sent_tokenize(comment)\n",
    "    filtered_sentences = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        # Tokenize using spaCy to keep punctuation and proper casing\n",
    "        doc = nlp(sent)\n",
    "        filtered = \" \".join([token.text for token in doc if token.text.lower() not in stop_words])\n",
    "        # Remove short garbage sentences (like one or two words)\n",
    "        if len(filtered.split()) > 3:\n",
    "            filtered_sentences.append(filtered)\n",
    "\n",
    "    return filtered_sentences\n",
    "\n",
    "# Sentencize and flatten\n",
    "df[\"sentences\"] = df[\"cleaned_comment\"].apply(sentencize_and_filter)\n",
    "flattened_sentences = [sentence for sublist in df[\"sentences\"] for sentence in sublist]\n",
    "\n",
    "# Final output dataframe\n",
    "df_sentences = pd.DataFrame(flattened_sentences, columns=[\"sentence\"])\n",
    "print(\"✅ Final sentence dataframe shape:\", df_sentences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences.to_pickle(\"../data/df_sentences.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
